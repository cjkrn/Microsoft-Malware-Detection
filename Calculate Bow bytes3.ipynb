{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(u'nbAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from multiprocessing import Process# this is used for multithreading\n",
    "import multiprocessing\n",
    "import codecs# this is used for file operations \n",
    "import random as r\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01jsnpXSAlgw6aPeDxrU.txt',\n",
       " '02zcUmKV16Lya5xqnPGB.txt',\n",
       " '04sJnMaORYc1SV5pKjrP.txt',\n",
       " '06aLOj8EUXMByS423sum.txt',\n",
       " '08BX5Slp2I1FraZWbc6j.txt',\n",
       " '0akIgwhWHYm1dzsNqBFx.txt',\n",
       " '0aVxkvmflEizUBG2rMT4.txt',\n",
       " '0BKcmNv4iGY2hsVSaXJ6.txt',\n",
       " '0cfIE39ihRNo2rkZOw5H.txt',\n",
       " '0CzL6rfwaTqGOu9eghBt.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir('b3/')\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder = 1 and length 1359\n",
      "Folder = 2 and length 1359\n",
      "Folder = 3 and length 1359\n",
      "Folder = 4 and length 1359\n",
      "Folder = 5 and length 1358\n",
      "Folder = 6 and length 1358\n",
      "Folder = 7 and length 1358\n",
      "Folder = 8 and length 1358\n"
     ]
    }
   ],
   "source": [
    "# No of byte files per folder\n",
    "for folder in range(1,9):\n",
    "    f = os.listdir('b'+str(folder)+'/')\n",
    "    print(f'Folder = {folder} and length {len(f)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66049\n"
     ]
    }
   ],
   "source": [
    "# Applying the Multiprocessing method of ASM Files as Countvectorizer needs all the files in RAM and I have only 8gb RAM \n",
    "# Getting Bi-gram vocab\n",
    "\n",
    "vocab= \"00,01,02,03,04,05,06,07,08,09,0a,0b,0c,0d,0e,0f,10,11,12,13,14,15,16,17,18,19,1a,1b,1c,1d,1e,1f,20,21,22,23,24,25,26,27,28,29,2a,2b,2c,2d,2e,2f,30,31,32,33,34,35,36,37,38,39,3a,3b,3c,3d,3e,3f,40,41,42,43,44,45,46,47,48,49,4a,4b,4c,4d,4e,4f,50,51,52,53,54,55,56,57,58,59,5a,5b,5c,5d,5e,5f,60,61,62,63,64,65,66,67,68,69,6a,6b,6c,6d,6e,6f,70,71,72,73,74,75,76,77,78,79,7a,7b,7c,7d,7e,7f,80,81,82,83,84,85,86,87,88,89,8a,8b,8c,8d,8e,8f,90,91,92,93,94,95,96,97,98,99,9a,9b,9c,9d,9e,9f,a0,a1,a2,a3,a4,a5,a6,a7,a8,a9,aa,ab,ac,ad,ae,af,b0,b1,b2,b3,b4,b5,b6,b7,b8,b9,ba,bb,bc,bd,be,bf,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,ca,cb,cc,cd,ce,cf,d0,d1,d2,d3,d4,d5,d6,d7,d8,d9,da,db,dc,dd,de,df,e0,e1,e2,e3,e4,e5,e6,e7,e8,e9,ea,eb,ec,ed,ee,ef,f0,f1,f2,f3,f4,f5,f6,f7,f8,f9,fa,fb,fc,fd,fe,ff,??\"\n",
    "    \n",
    "bi_gram_vocab=[]\n",
    "for index, hex in enumerate(vocab.split(',')):\n",
    "    for j in vocab.split(\",\"):\n",
    "        bi_gram_vocab.append(hex+' '+ j)\n",
    "print(len(bi_gram_vocab))\n",
    "\n",
    "# # Converting it into a dask delayed object\n",
    "# bigram_dict={}\n",
    "# for index, hex in enumerate(bi_gram_vocab):\n",
    "#     bigram_dict[hex]= index    \n",
    "# print(bigram_dict['00 00'])\n",
    "# print(len(bigram_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOING IT THE MULTIPROCESSING WAY\n",
    "\n",
    "# First 4 have 1359 byte files each and last 4 have 1358 files each\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def f1():\n",
    "    print('Starting and running process no 1')\n",
    "    vect = CountVectorizer(lowercase=False,vocabulary=bi_gram_vocab ,ngram_range=(2,2))\n",
    "    b1_matrix = csr_matrix((1359,len(bi_gram_vocab)))\n",
    "    \n",
    "    for i,file in tqdm(enumerate(os.listdir('b1/'))):\n",
    "        f = open('b1/' + file)\n",
    "        b1_matrix[i,:] += csr_matrix(vect.fit_transform([f.read().replace('\\n',' ').lower()]))\n",
    "        f.close()\n",
    "\n",
    "    # Making a matrix and making it the column of b1_data dataframe \n",
    "    b1_matrix = b1_matrix.todense()\n",
    "    b1_data = pd.DataFrame(b1_matrix,columns=bi_gram_vocab)\n",
    "    b1_data['ID'] = os.listdir('b1/')\n",
    "    b1_data.to_pickle('b1_data.pkl')\n",
    "    # with open('/content/gdrive/My Drive/Assignments AAIC/Assignment 16 Malware/Final/bytes/b1_data', 'wb') as files:\n",
    "    #     joblib.dump(b1_data, files)\n",
    "    \n",
    "    \n",
    "def f2():\n",
    "    print('Starting and running process no 2')\n",
    "    vect = CountVectorizer(lowercase=False,vocabulary=bi_gram_vocab,ngram_range=(2,2))\n",
    "    b2_matrix = csr_matrix((1359,len(bi_gram_vocab)))\n",
    "    \n",
    "    for i,file in tqdm(enumerate(os.listdir('b2/'))):\n",
    "        f = open('b2/' + file)\n",
    "        b2_matrix[i,:] += csr_matrix(vect.fit_transform([f.read().replace('\\n',' ').lower()]))\n",
    "        f.close()\n",
    "        \n",
    "    b2_matrix = b2_matrix.todense()\n",
    "    b2_data = pd.DataFrame(b2_matrix,columns=bi_gram_vocab)\n",
    "    b2_data['ID'] = os.listdir('b2/')\n",
    "    b2_data.to_pickle('b2_data.pkl')\n",
    "    # with open('/content/gdrive/My Drive/Assignments AAIC/Assignment 16 Malware/Final/bytes/b2_data', 'wb') as files:\n",
    "    #     joblib.dump(b2_data, files)\n",
    "\n",
    "def f3():\n",
    "    print('Starting and running process no 3')\n",
    "    vect = CountVectorizer(lowercase=False,vocabulary=bi_gram_vocab,ngram_range=(2,2))\n",
    "    b3_matrix = csr_matrix((1359,len(bi_gram_vocab)))\n",
    "    \n",
    "    for i,file in tqdm(enumerate(os.listdir('b3/'))):\n",
    "        f = open('b3/' + file)\n",
    "        b3_matrix[i,:] += csr_matrix(vect.fit_transform([f.read().replace('\\n',' ').lower()]))\n",
    "        f.close()\n",
    "        \n",
    "    b3_matrix = b3_matrix.todense()\n",
    "    b3_data = pd.DataFrame(b3_matrix,columns=bi_gram_vocab)\n",
    "    b3_data['ID'] = os.listdir('b3/')\n",
    "    b3_data.to_pickle('b3_data.pkl')\n",
    "    # with open('/content/gdrive/My Drive/Assignments AAIC/Assignment 16 Malware/Final/bytes/b3_data', 'wb') as files:\n",
    "    #     joblib.dump(b3_data, files)\n",
    "    \n",
    "def f4():\n",
    "    print('Starting and running process no 4')\n",
    "    vect = CountVectorizer(lowercase=False,vocabulary=bi_gram_vocab,ngram_range=(2,2))\n",
    "    b4_matrix = csr_matrix((1359,len(bi_gram_vocab)))\n",
    "    \n",
    "    for i,file in tqdm(enumerate(os.listdir('b4/'))):\n",
    "        f = open('b4/' + file)\n",
    "        b4_matrix[i,:] += csr_matrix(vect.fit_transform([f.read().replace('\\n',' ').lower()]))\n",
    "        f.close()\n",
    "        \n",
    "    b4_matrix = b4_matrix.todense()\n",
    "    b4_data = pd.DataFrame(b4_matrix,columns=bi_gram_vocab)\n",
    "    b4_data['ID'] = os.listdir('b4/')\n",
    "    b4_data.to_pickle('b4_data.pkl')\n",
    "    # with open('/content/gdrive/My Drive/Assignments AAIC/Assignment 16 Malware/Final/bytes/b4_data', 'wb') as files:\n",
    "    #     joblib.dump(b4_data, files)\n",
    "    \n",
    "    \n",
    "def f5():\n",
    "    print('Starting and running process no 5')\n",
    "    vect = CountVectorizer(lowercase=False,vocabulary=bi_gram_vocab,ngram_range=(2,2))\n",
    "    b5_matrix = csr_matrix((1358,len(bi_gram_vocab)))\n",
    "    \n",
    "    for i,file in tqdm(enumerate(os.listdir('b5/'))):\n",
    "        f = open('b5/' + file)\n",
    "        b5_matrix[i,:] += csr_matrix(vect.fit_transform([f.read().replace('\\n',' ').lower()]))\n",
    "        f.close()\n",
    "        \n",
    "    b5_matrix = b5_matrix.todense()\n",
    "    b5_data = pd.DataFrame(b5_matrix,columns=bi_gram_vocab)\n",
    "    b5_data['ID'] = os.listdir('b5/')\n",
    "    b5_data.to_pickle('b5_data.pkl')\n",
    "    # with open('/content/gdrive/My Drive/Assignments AAIC/Assignment 16 Malware/Final/bytes/b5_data', 'wb') as files:\n",
    "    #     joblib.dump(b5_data, files)\n",
    "    \n",
    "def f6():\n",
    "    print('Starting and running process no 6')\n",
    "    vect = CountVectorizer(lowercase=False,vocabulary=bi_gram_vocab,ngram_range=(2,2))\n",
    "    b6_matrix = csr_matrix((1358,len(bi_gram_vocab)))\n",
    "    \n",
    "    for i,file in tqdm(enumerate(os.listdir('b6/'))):\n",
    "        f = open('b6/' + file)\n",
    "        b6_matrix[i,:] += csr_matrix(vect.fit_transform([f.read().replace('\\n',' ').lower()]))\n",
    "        f.close()\n",
    "        \n",
    "    b6_matrix = b6_matrix.todense()\n",
    "    b6_data = pd.DataFrame(b6_matrix,columns=bi_gram_vocab)\n",
    "    b6_data['ID'] = os.listdir('b6/')\n",
    "    b6_data.to_pickle('b6_data.pkl')\n",
    "    # with open('/content/gdrive/My Drive/Assignments AAIC/Assignment 16 Malware/Final/bytes/b6_data', 'wb') as files:\n",
    "    #     joblib.dump(b6_data, files)\n",
    "    \n",
    "def f7():\n",
    "    print('Starting and running process no 7')\n",
    "    vect = CountVectorizer(lowercase=False,vocabulary=bi_gram_vocab,ngram_range=(2,2))\n",
    "    b7_matrix = csr_matrix((1358,len(bi_gram_vocab)))\n",
    "    \n",
    "    for i,file in tqdm(enumerate(os.listdir('b7/'))):\n",
    "        f = open('b7/' + file)\n",
    "        b7_matrix[i,:] += csr_matrix(vect.fit_transform([f.read().replace('\\n',' ').lower()]))\n",
    "        f.close()\n",
    "        \n",
    "    b7_matrix = b7_matrix.todense()\n",
    "    b7_data = pd.DataFrame(b7_matrix,columns=bi_gram_vocab)\n",
    "    b7_data['ID'] = os.listdir('b7/')\n",
    "    b7_data.to_pickle('b7_data.pkl')\n",
    "    # with open('/content/gdrive/My Drive/Assignments AAIC/Assignment 16 Malware/Final/bytes/b7_data', 'wb') as files:\n",
    "    #     joblib.dump(b7_data, files)\n",
    "\n",
    "def f8():\n",
    "    print('Starting and running process no 8')\n",
    "    vect = CountVectorizer(lowercase=False,vocabulary=bi_gram_vocab,ngram_range=(2,2))\n",
    "    b8_matrix = csr_matrix((1358,len(bi_gram_vocab)))\n",
    "    \n",
    "    for i,file in tqdm(enumerate(os.listdir('b8/'))):\n",
    "        f = open('b8/' + file)\n",
    "        b8_matrix[i,:] += csr_matrix(vect.fit_transform([f.read().replace('\\n',' ').lower()]))\n",
    "        f.close()\n",
    "        \n",
    "    b8_matrix = b8_matrix.todense()\n",
    "    b8_data = pd.DataFrame(b8_matrix,columns=bi_gram_vocab)\n",
    "    b8_data['ID'] = os.listdir('b8/')\n",
    "    b8_data.to_pickle('b8_data.pkl')\n",
    "    # with open('/content/gdrive/My Drive/Assignments AAIC/Assignment 16 Malware/Final/bytes/b8_data', 'wb') as files:\n",
    "    #     joblib.dump(b8_data, files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting and running process no 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1359it [50:45,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "f3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7a650a0512ab63a57e877f4b11948ac2343f5e7259e8b5d406ecff5191ba340"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('malware')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
